{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiple Choice","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyZK8lFrOAozRlx8D5v21Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"9oDxqOTfyaOK"},"source":["! pip install datasets transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y19WaBo23lLs"},"source":["!apt install git-lfs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pELbVoR3pSU"},"source":["import transformers\n","\n","print(transformers.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVG3i8eH3qWM"},"source":["model_checkpoint = \"bert-base-uncased\"\n","batch_size = 16\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"py96yAM03s2Z"},"source":["from datasets import load_dataset, load_metric\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fm5F-to23tiX"},"source":["datasets\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hi0uDlWK3uVI"},"source":["datasets[\"train\"][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAkACruz3vbb"},"source":["from datasets import ClassLabel\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnCjp9mB3wqw"},"source":["show_random_elements(datasets[\"train\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5J2eBKc3yuB"},"source":["def show_one(example):\n","    print(f\"Context: {example['sent1']}\")\n","    print(f\"  A - {example['sent2']} {example['ending0']}\")\n","    print(f\"  B - {example['sent2']} {example['ending1']}\")\n","    print(f\"  C - {example['sent2']} {example['ending2']}\")\n","    print(f\"  D - {example['sent2']} {example['ending3']}\")\n","    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1LGAWVI3zjB"},"source":["show_one(datasets[\"train\"][0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovbL-OwA30b2"},"source":["show_one(datasets[\"train\"][15])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRf1XPET32Aa"},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6CYTSwi3244"},"source":["tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvWWdGJ434JS"},"source":["ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n","\n","def preprocess_function(examples):\n","    # Repeat each first sentence four times to go with the four possibilities of second sentences.\n","    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n","    # Grab all second sentences possible for each context.\n","    question_headers = examples[\"sent2\"]\n","    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n","    \n","    # Flatten everything\n","    first_sentences = sum(first_sentences, [])\n","    second_sentences = sum(second_sentences, [])\n","    \n","    # Tokenize\n","    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n","    # Un-flatten\n","    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4-qcsKN35uF"},"source":["examples = datasets[\"train\"][:5]\n","features = preprocess_function(examples)\n","print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QC3S6nxU37Ou"},"source":["idx = 3\n","[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JE6A36n738OA"},"source":["show_one(datasets[\"train\"][3])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZFXWN8R39Ep"},"source":["encoded_datasets = datasets.map(preprocess_function, batched=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DOf4X7i3-Hm"},"source":["from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","\n","model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UA-H00_23_hR"},"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-swag\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=True,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoZWPHp_4B6o"},"source":["from dataclasses import dataclass\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from typing import Optional, Union\n","import torch\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator that will dynamically pad the inputs for multiple choice received.\n","    \"\"\"\n","\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features):\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0][\"input_ids\"])\n","        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n","        flattened_features = sum(flattened_features, [])\n","        \n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        \n","        # Un-flatten\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        # Add back labels\n","        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlN_wDM24Cv3"},"source":["accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n","features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n","batch = DataCollatorForMultipleChoice(tokenizer)(features)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQ7USC_i4Dbu"},"source":["[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(4)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyDNa1QS4EZq"},"source":["show_one(datasets[\"train\"][8])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Vfn05S24FM0"},"source":["import numpy as np\n","\n","def compute_metrics(eval_predictions):\n","    predictions, label_ids = eval_predictions\n","    preds = np.argmax(predictions, axis=1)\n","    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yg2w98gY4GMM"},"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=encoded_datasets[\"train\"],\n","    eval_dataset=encoded_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer),\n","    compute_metrics=compute_metrics,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-Kx2tX94HAK"},"source":["trainer.train()\n"],"execution_count":null,"outputs":[]}]}