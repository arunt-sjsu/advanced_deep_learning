{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question Answering","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMuAQdwjtYwSd3eLQiIMkqc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cN2GUtsqyY3a"},"source":["#! pip install datasets transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDllfaAV4b4B"},"source":["# !apt install git-lfs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8FI5mmm4eim"},"source":["import transformers\n","\n","print(transformers.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Tu9pdVn4hed"},"source":["# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n","# answers are allowed or not).\n","squad_v2 = False\n","model_checkpoint = \"distilbert-base-uncased\"\n","batch_size = 16\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4whJwlHp4iXi"},"source":["from datasets import load_dataset, load_metric\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4B3kgW34jCQ"},"source":["datasets\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7mc1jS4s4kEU"},"source":["datasets[\"train\"][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIC-wv4f4lZ-"},"source":["from datasets import ClassLabel, Sequence\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n","            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n","    display(HTML(df.to_html()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgjKPVoE4mM9"},"source":["show_random_elements(datasets[\"train\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Phm-3oBG4pSN"},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAuyNrgV40IX"},"source":["import transformers\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MlazJIV41MR"},"source":["tokenizer(\"What is your name?\", \"My name is Sylvain.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz1pvAtm42NY"},"source":["max_length = 384 # The maximum length of a feature (question and context)\n","doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVo8U84243AU"},"source":["len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgVhSZCp44p3"},"source":["len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL1k3Fa64523"},"source":["tokenized_example = tokenizer(\n","    example[\"question\"],\n","    example[\"context\"],\n","    max_length=max_length,\n","    truncation=\"only_second\",\n","    return_overflowing_tokens=True,\n","    stride=doc_stride\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYu6oBqE46uf"},"source":["[len(x) for x in tokenized_example[\"input_ids\"]]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yQfLafA47eH"},"source":["for x in tokenized_example[\"input_ids\"][:2]:\n","    print(tokenizer.decode(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dx_T7NDs49Kg"},"source":["tokenized_example = tokenizer(\n","    example[\"question\"],\n","    example[\"context\"],\n","    max_length=max_length,\n","    truncation=\"only_second\",\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n","    stride=doc_stride\n",")\n","print(tokenized_example[\"offset_mapping\"][0][:100])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjokLUA44-Um"},"source":["first_token_id = tokenized_example[\"input_ids\"][0][1]\n","offsets = tokenized_example[\"offset_mapping\"][0][1]\n","print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZX_gnVE4_H-"},"source":["sequence_ids = tokenized_example.sequence_ids()\n","print(sequence_ids)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9u6Inuds5AnU"},"source":["answers = example[\"answers\"]\n","start_char = answers[\"answer_start\"][0]\n","end_char = start_char + len(answers[\"text\"][0])\n","\n","# Start token index of the current span in the text.\n","token_start_index = 0\n","while sequence_ids[token_start_index] != 1:\n","    token_start_index += 1\n","\n","# End token index of the current span in the text.\n","token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n","while sequence_ids[token_end_index] != 1:\n","    token_end_index -= 1\n","\n","# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","offsets = tokenized_example[\"offset_mapping\"][0]\n","if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","    # Move the token_start_index and token_end_index to the two ends of the answer.\n","    # Note: we could go after the last offset if the answer is the last word (edge case).\n","    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","        token_start_index += 1\n","    start_position = token_start_index - 1\n","    while offsets[token_end_index][1] >= end_char:\n","        token_end_index -= 1\n","    end_position = token_end_index + 1\n","    print(start_position, end_position)\n","else:\n","    print(\"The answer is not in this feature.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7eHW7RF5Blm"},"source":["print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n","print(answers[\"text\"][0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQ_0KI5g5Cey"},"source":["pad_on_right = tokenizer.padding_side == \"right\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dGPNR5L5ETh"},"source":["def prepare_train_features(examples):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    # The offset mappings will give us a map from token to character position in the original context. This will\n","    # help us compute the start_positions and end_positions.\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    # Let's label those examples!\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","\n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[\"answers\"][sample_index]\n","        # If no answers are given, set the cls_index as answer.\n","        if len(answers[\"answer_start\"]) == 0:\n","            tokenized_examples[\"start_positions\"].append(cls_index)\n","            tokenized_examples[\"end_positions\"].append(cls_index)\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            # Start token index of the current span in the text.\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n","                token_start_index += 1\n","\n","            # End token index of the current span in the text.\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n","                token_end_index -= 1\n","\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","            else:\n","                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n","                # Note: we could go after the last offset if the answer is the last word (edge case).\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","\n","    return tokenized_examples\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgxbgtKC5FG-"},"source":["features = prepare_train_features(datasets['train'][:5])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtX4rkL55F12"},"source":["tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZYCdZC75Jwp"},"source":["from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d9YmJIv5LfJ"},"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-squad\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jnM_yWs5MvR"},"source":["from transformers import default_data_collator\n","\n","data_collator = default_data_collator\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_GMceDo5PGO"},"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3PUqzs-5UOk"},"source":["trainer.train()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiu2_pbM5Xqg"},"source":["trainer.save_model(\"test-squad-trained\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FTcvCd-5ZJf"},"source":["import torch\n","\n","for batch in trainer.get_eval_dataloader():\n","    break\n","batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n","with torch.no_grad():\n","    output = trainer.model(**batch)\n","output.keys()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6icyMGF5aMo"},"source":["output.start_logits.shape, output.end_logits.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jf1Sw6h35bCh"},"source":["output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-WurZ-15bIw"},"source":["n_best_size = 20\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQ_bwHoD5fZp"},"source":["import numpy as np\n","\n","start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","# Gather the indices the best start/end logits:\n","start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n","            valid_answers.append(\n","                {\n","                    \"score\": start_logits[start_index] + end_logits[end_index],\n","                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n","                }\n","            )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOf8BR0V5hKe"},"source":["def prepare_validation_features(examples):\n","    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n","    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n","    # left whitespace\n","    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n","\n","    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\" if pad_on_right else \"context\"],\n","        examples[\"context\" if pad_on_right else \"question\"],\n","        truncation=\"only_second\" if pad_on_right else \"only_first\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","\n","    # We keep the example_id that gave us this feature and we will store the offset mappings.\n","    tokenized_examples[\"example_id\"] = []\n","\n","    for i in range(len(tokenized_examples[\"input_ids\"])):\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","        context_index = 1 if pad_on_right else 0\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n","\n","        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n","        # position is part of the context or not.\n","        tokenized_examples[\"offset_mapping\"][i] = [\n","            (o if sequence_ids[k] == context_index else None)\n","            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n","        ]\n","\n","    return tokenized_examples\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yfQSYV-5ieD"},"source":["validation_features = datasets[\"validation\"].map(\n","    prepare_validation_features,\n","    batched=True,\n","    remove_columns=datasets[\"validation\"].column_names\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMgKfYr55jT8"},"source":["raw_predictions = trainer.predict(validation_features)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fhaRLNu5kZJ"},"source":["validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2r3jJZk5lTM"},"source":["max_answer_length = 30\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ThnFnju5mtn"},"source":["start_logits = output.start_logits[0].cpu().numpy()\n","end_logits = output.end_logits[0].cpu().numpy()\n","offset_mapping = validation_features[0][\"offset_mapping\"]\n","# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n","# an example index\n","context = datasets[\"validation\"][0][\"context\"]\n","\n","# Gather the indices the best start/end logits:\n","start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","valid_answers = []\n","for start_index in start_indexes:\n","    for end_index in end_indexes:\n","        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n","        # to part of the input_ids that are not in the context.\n","        if (\n","            start_index >= len(offset_mapping)\n","            or end_index >= len(offset_mapping)\n","            or offset_mapping[start_index] is None\n","            or offset_mapping[end_index] is None\n","        ):\n","            continue\n","        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","            continue\n","        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n","            start_char = offset_mapping[start_index][0]\n","            end_char = offset_mapping[end_index][1]\n","            valid_answers.append(\n","                {\n","                    \"score\": start_logits[start_index] + end_logits[end_index],\n","                    \"text\": context[start_char: end_char]\n","                }\n","            )\n","\n","valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n","valid_answers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgSEcvNm5oW6"},"source":["datasets[\"validation\"][0][\"answers\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ix9ZMm_t5oxM"},"source":["import collections\n","\n","examples = datasets[\"validation\"]\n","features = validation_features\n","\n","example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","features_per_example = collections.defaultdict(list)\n","for i, feature in enumerate(features):\n","    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-s1u6-Ca5sE_"},"source":["from tqdm.auto import tqdm\n","\n","def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n","    all_start_logits, all_end_logits = raw_predictions\n","    # Build a map example to its corresponding features.\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","    features_per_example = collections.defaultdict(list)\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    # The dictionaries we have to fill.\n","    predictions = collections.OrderedDict()\n","\n","    # Logging.\n","    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    # Let's loop over all the examples!\n","    for example_index, example in enumerate(tqdm(examples)):\n","        # Those are the indices of the features associated to the current example.\n","        feature_indices = features_per_example[example_index]\n","\n","        min_null_score = None # Only used if squad_v2 is True.\n","        valid_answers = []\n","        \n","        context = example[\"context\"]\n","        # Looping through all the features associated to the current example.\n","        for feature_index in feature_indices:\n","            # We grab the predictions of the model for this feature.\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","            # This is what will allow us to map some the positions in our logits to span of texts in the original\n","            # context.\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","\n","            # Update minimum null prediction.\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            if min_null_score is None or min_null_score < feature_null_score:\n","                min_null_score = feature_null_score\n","\n","            # Go through all possibilities for the `n_best_size` greater start and end logits.\n","            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n","                    # to part of the input_ids that are not in the context.\n","                    if (\n","                        start_index >= len(offset_mapping)\n","                        or end_index >= len(offset_mapping)\n","                        or offset_mapping[start_index] is None\n","                        or offset_mapping[end_index] is None\n","                    ):\n","                        continue\n","                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        \n","        if len(valid_answers) > 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n","            # failure.\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","        \n","        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n","        if not squad_v2:\n","            predictions[example[\"id\"]] = best_answer[\"text\"]\n","        else:\n","            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n","            predictions[example[\"id\"]] = answer\n","\n","    return predictions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QC-wDstP5tc5"},"source":["final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-HZAUAo5ude"},"source":["metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-Ipofu-5vpK"},"source":["if squad_v2:\n","    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n","else:\n","    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n","references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n","metric.compute(predictions=formatted_predictions, references=references)\n"],"execution_count":null,"outputs":[]}]}