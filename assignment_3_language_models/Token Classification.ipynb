{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Token Classification","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMSd01yO8pNmZDZmqQVibE0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eoxv61x2yWH2","executionInfo":{"status":"ok","timestamp":1635574407395,"user_tz":420,"elapsed":26630,"user":{"displayName":"Arun Talkad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3t7SxjG9qC7EWFbrBf5Umx_aS18DfE6KqZTDL=s64","userId":"18230572489592832102"}},"outputId":"4e84fb08-e0fe-4e65-ef43-9b544f74c636"},"source":["! pip install datasets transformers seqeval\n","! apt install git-lfs"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n","\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 174 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 184 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 204 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 215 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 225 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 256 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 266 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 276 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 286 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 290 kB 32.2 MB/s \n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 36.0 MB/s \n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 32.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 27.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.19\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 31.5 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.13)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 38.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 33.0 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 32.2 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 63.4 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 27.5 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=e65d3dc0827689755f0fdaa94da825e1d591cdae036297163d17e8eda67910f9\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: multidict, yarl, async-timeout, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, seqeval, datasets\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 seqeval-1.2.2 tokenizers-0.10.3 transformers-4.12.2 xxhash-2.0.2 yarl-1.7.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  git-lfs\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 2,129 kB of archives.\n","After this operation, 7,662 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n","Fetched 2,129 kB in 1s (2,883 kB/s)\n","Selecting previously unselected package git-lfs.\n","(Reading database ... 155062 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n","Unpacking git-lfs (2.3.4-1) ...\n","Setting up git-lfs (2.3.4-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"id":"sbvqIMfNy5H9"},"source":["import transformers\n","\n","print(transformers.__version__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7UDKAng1M6d"},"source":["task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n","model_checkpoint = \"distilbert-base-uncased\"\n","batch_size = 16\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uYsn4hh1Puf"},"source":["from datasets import load_dataset, load_metric\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXncDq0V1RWC"},"source":["datasets = load_dataset(\"conll2003\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckw0P2tV1SPI"},"source":["datasets\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5kKPskF1TcF"},"source":["datasets[\"train\"][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5_LL7sQ1UY8"},"source":["datasets[\"train\"].features[f\"ner_tags\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VqCgjb51V9o"},"source":["label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n","label_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NX6j5Te1XoU"},"source":["from datasets import ClassLabel, Sequence\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n","    picks = []\n","    for _ in range(num_examples):\n","        pick = random.randint(0, len(dataset)-1)\n","        while pick in picks:\n","            pick = random.randint(0, len(dataset)-1)\n","        picks.append(pick)\n","    \n","    df = pd.DataFrame(dataset[picks])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n","            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n","    display(HTML(df.to_html()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ap9LG1Dp1Ylo"},"source":["show_random_elements(datasets[\"train\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATdjSP6b1aIT"},"source":["from transformers import AutoTokenizer\n","    \n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jc1-c3ul1bFI"},"source":["import transformers\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SmlXLQW1b_d"},"source":["tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTlLNpvV1czL"},"source":["example = datasets[\"train\"][4]\n","print(example[\"tokens\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDFrWy_L1eV2"},"source":["tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","print(tokens)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3fSDkLT1fDw"},"source":["len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qILrGDFX1f7Z"},"source":["print(tokenized_input.word_ids())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkg9enBM1g4Y"},"source":["word_ids = tokenized_input.word_ids()\n","aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n","print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABa1GGTN1hu5"},"source":["label_all_tokens = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQsrGmEn1jJ2"},"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"{task}_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx])\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            else:\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCgpunWc1kAu"},"source":["tokenize_and_align_labels(datasets['train'][:5])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovFzsmHT1kyx"},"source":["tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNjjEdPx1m7M"},"source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzbFqfXR1oNd"},"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{task}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYlEzypn1pdA"},"source":["from transformers import DataCollatorForTokenClassification\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21sTOoYz1pi3"},"source":["labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n","metric.compute(predictions=[labels], references=[labels])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HbMXNtQK1tLP"},"source":["import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lx_WgrD51uYA"},"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJKzTU1I1vat"},"source":["trainer.train()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT7qx9Ng1wgI"},"source":["predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n","predictions = np.argmax(predictions, axis=2)\n","\n","# Remove ignored index (special tokens)\n","true_predictions = [\n","    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","true_labels = [\n","    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","results\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_ZYUf1t1yh7"},"source":["from transformers import AutoModelForTokenClassification\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"sgugger/my-awesome-model\")\n"],"execution_count":null,"outputs":[]}]}